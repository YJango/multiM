{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "# read word2vec wiki-news-300d-1M.vec\n",
    "#from gensim.models import KeyedVectors\n",
    "#model = KeyedVectors.load_word2vec_format('/USERS/d8182103/facebook_data/embedding/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ti_du_xia_jiang_xun_lian_fa.md','r') as f:\n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ti_du_xia_jiang_xun_lian_fa.txt','w') as f:\n",
    "    for e,l in enumerate(lines.split('$$')):\n",
    "        if e%2!=0:\n",
    "            f.write('`$'+l+'$`')\n",
    "        else:\n",
    "            f.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 为什么神经网络能够识别\\n\\n为了研究神经网络，我们必须要对什么网络是什么有一个更直观的认识。\\n##基本变换：层\\n***一、神经网络是由一层一层构建的，那么每层究竟在做什么？***\\n\\n - **数学式子**：`$\\\\vec{y}= a(W\\\\cdot\\\\vec{x} + {b})$`，其中`$\\\\vec{x}$`是输入向量，`$\\\\vec{y}$`是输出向量，`$\\\\vec{b}$`是偏移向量，`$W$`是权重矩阵，`$a()$`是激活函数。每一层仅仅是把输入`$\\\\vec{x}$`经过如此简单的操作得到`$\\\\vec{y}$`。\\n - **数学理解**：通过如下5种对输入空间（输入向量的集合）的操作，完成![](transform.png)的变换(矩阵的行空间到列空间)。\\n \\n **注**：用“空间”二字是指被分类的并不是单个事物，而是**一类**事物。空间是指这类事物所有个体的集合。\\n \\n      - **1.** 升维/降维   \\n      - **2.** 放大/缩小   \\n      - **3.** 旋转  \\n      - **4.** 平移   \\n      - **5.** “弯曲”\\n      \\n      ![](/assets/1layer.gif)\\n      \\n      这5种操作中，1,2,3的操作由`$W\\\\cdot\\\\vec{x}$`完成，4的操作是由`$+ \\\\vec{b}$`完成，5的操作则是由`$a()$`来实现。\\n      \\n\\n\\n>每层的数学理解：**用线性变换跟随着非线性变化，将输入空间投向另一个空间**。\\n\\n\\n - **物理理解**：对 `$W\\\\cdot\\\\vec{x}$` 的理解就是**通过组合形成新物质**。`$a()$`又符合了我们所处的世界都是非线性的特点。\\n  - **假想情景：**`$\\\\vec{x}$`是二维向量，维度是碳原子和氧原子的数量 `$[C； O]$`，数值且定为`$[1； 1]$`。若确定`$\\\\vec{y}$`是三维向量，就会形成如下网络的形状 (神经网络的每个节点表示一个维度)。通过改变权重的值，可以获得若干个不同物质。右侧的节点数决定了想要获得多少种不同的新物质（矩阵的行数）。\\n  \\n    ![二维--->三维](http://img.blog.csdn.net/20161005175753956)\\n  \\n      - 若权重`$W$`的数值如(1)，那么网络的输出`$\\\\vec{y}$`就会是三个新物质，[二氧化碳，臭氧，一氧化碳]。  \\n        `$ \\\\left[\\n \\\\begin{matrix}\\n   CO_{2}\\\\\\\\\\n   O_{3}\\\\\\\\\\n   CO\\n  \\\\end{matrix}\\n  \\\\right]=\\n \\\\left[\\n \\\\begin{matrix}\\n   1 & 2 \\\\\\\\\\n   0 & 3\\\\\\\\\\n   1 & 1\\n  \\\\end{matrix}\\n  \\\\right] \\\\cdot \\\\left[\\n \\\\begin{matrix}\\n   C \\\\\\\\\\n   O \\\\\\\\\\n  \\\\end{matrix}\\n  \\\\right]\\\\tag{1}$`\\n  \\n      - 若减少右侧的一个节点，并改变权重`$W$`至(2)，那输出`$\\\\vec{y}$`就会是两个新物质，`$[ O_{0.3} ；CO_{1.5}]$`。\\n        `$ \\\\left[\\n \\\\begin{matrix}\\n    O_{0.3}\\\\\\\\\\n   CO_{1.5}\\\\\\\\\\n  \\\\end{matrix}\\n  \\\\right]=\\n \\\\left[\\n \\\\begin{matrix}\\n   0& 0.3 \\\\\\\\\\n   1 & 1.5\\\\\\\\\\n  \\\\end{matrix}\\n  \\\\right] \\\\cdot \\\\left[\\n \\\\begin{matrix}\\n   C \\\\\\\\\\n   O \\\\\\\\\\n  \\\\end{matrix}\\n  \\\\right]\\\\tag{2}$`\\n  \\n      - 若再加一层，就是再次通过组合`$[CO_{2}；O_{3}；CO]$`这三种基础物质，形成若干个更高层的物质。\\n      - 若希望通过层网络能够从[C, O]空间转变到`$[CO_{2}；O_{3}；CO]$`空间的话，那么网络的学习过程就是将`$W$`的数值变成尽可能接近(1)的过程 。\\n      - 重要的是这种组合思想，组合成的东西在神经网络中并不需要有物理意义,可以是抽象概念。\\n \\n  \\n>每层神经网络的物理理解：**通过现有的不同物质的组合形成新物质**。\\n\\n\\n\\n##理解视角：\\n***二、现在我们知道了每一层的行为，但这种行为又是如何完成识别任务的呢？***\\n###数学视角：“线性可分”\\n\\n - **一维情景**：以分类为例，当要分类正数、负数、零，三类的时候，一维空间的直线可以找到两个超平面（比当前空间低一维的子空间。当前空间是直线的话，超平面就是点）分割这三类。但面对像分类奇数和偶数无法找到可以区分它们的点的时候，我们借助 x % 2（除2取余）的转变，把x变换到另一个空间下来比较0和非0，从而分割奇偶数。\\n \\n ![这里写图片描述](http://img.blog.csdn.net/20161005204500151)\\n \\n - **二维情景**：平面的四个象限也是线性可分。但下图的红蓝两条线就无法找到一超平面去分割。\\n \\n ![](/assets/simple2_linear.png)\\n\\n 神经网络的解决方法依旧是转换到另外一个空间下，用的是所说的**5种空间变换操作**。比如下图就是经过放大、平移、旋转、扭曲原二维空间后，在三维空间下就可以成功找到一个超平面分割红蓝两线 (同SVM的思路一样)。\\n \\n ![](/assets/simple2_1.png)\\n \\n 上面是一层神经网络可以做到的空间变化。若把`$\\\\vec{y}$` 当做新的输入再次用这5种操作进行第二遍空间变换的话，网络也就变为了二层。最终输出是`$\\\\vec{y}= a_{2}(W_{2}\\\\cdot(a_{1}(W_{1}\\\\cdot\\\\vec{x} + {b}_{1})) + {b}_{2})$`。设想当网络拥有很多层时，对原始输入空间的“扭曲力”会大幅增加，如下图，最终我们可以轻松找到一个超平面分割空间。\\n \\n ![](/assets/spiral-1-2-2-2-2-2-2-2.gif)\\n \\n 当然也有如下图失败的时候，关键在于“如何扭曲空间”。所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重`$W$`就**控制着如何变换空间**，我们最终需要的也就是训练好的神经网络的所有层的权重矩阵。。这里有非常棒的[可视化空间变换demo](http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)，**一定要**打开尝试并感受这种扭曲过程。\\n 更多内容请看[Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)。\\n \\n ![](/assets/spiral-2-2-2-2-2-2-2-2.gif)\\n\\n>线性可分视角：神经网络的学习就是**学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。**\\n>\\n>**增加节点数：增加维度，即增加线性转换能力。**\\n>\\n>**增加层数：增加激活函数的次数，即增加非线性转换次数。**\\n\\n\\n###物理视角：“物质组成”\\n\\n - **类比**：回想上文由碳氧原子通过不同组合形成若干分子的例子。若从分子层面继续迭代这种组合思想，可以形成DNA，细胞，组织，器官，最终可以形成一个完整的人。继续迭代还会有家庭，公司，国家等。这种现象在身边随处可见。并且原子的内部结构与太阳系又惊人的相似。不同层级之间都是以类似的几种规则再不断形成新物质。你也可能听过**分形学**这三个字。可通过观看[从1米到150亿光年](http://www.tudou.com/programs/view/o41zy0SeSS0)来感受自然界这种层级现象的普遍性。\\n \\n ![这里写图片描述](http://img.blog.csdn.net/20161006020759821)\\n \\n - **人脸识别情景**：我们可以模拟这种思想并应用在画面识别上。由像素组成菱角，再组成五官，最后到不同的人脸。每一层代表不同的物质层面 (如分子层)。而每层的`$W$`**存储着如何组合上一层的物质从而形成若干新物质**。\\n 如果我们完全掌握一架飞机是如何从分子开始一层一层形成的，拿到一堆分子后，我们就可以判断他们是否可以以此形成方式，形成一架飞机。\\n 附：[Tensorflow playground](http://playground.tensorflow.org/)展示了数据是如何“流动”的。\\n \\n  ![这里写图片描述](http://img.blog.csdn.net/20161006105827542)\\n  \\n  \\n> 物质组成视角：神经网络的学习过程就是**学习物质组成方式的过程。**\\n> \\n>**增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。**\\n> \\n>**增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。**\\n\\n\\n\\n\\n\\n\\n按照上文在理解视角中所述的观点，可以想出下面两条理由关于为什么更深的网络会更加容易识别，增加容纳变体（variation）（红苹果、绿苹果）的能力、鲁棒性（robust）。\\n\\n* **数学视角**：变体（variation）很多的分类的任务需要高度非线性的分割曲线。不断的利用那5种空间变换操作将原始输入空间像“捏橡皮泥一样”在高维空间下捏成更为线性可分/稀疏的形状：可视化空间变换。 \\n* **物理视角**：通过对“抽象概念”的判断来识别物体，而非细节。比如对“飞机”的判断，即便人类自己也无法用语言或者若干条规则来解释自己如何判断一个飞机。因为人脑中真正判断的不是是否“有机翼”、“能飞行”等细节现象，而是一个抽象概念。层数越深，这种概念就越抽象，所能涵盖的变异体就越多，就可以容纳战斗机，客机等很多种不同种类的飞机。\\n\\n然而这最多指解释了为何神经网络有效，并没有接触到核心问题：为何深层神经网络更有效'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter text on char level\n",
    "def textfilter(x):\n",
    "    # replace [inaudible 00:+] to inaudible\n",
    "    x = re.sub(r'\\[[^:]*(\\d\\d:\\d\\d|\\d\\d:\\d\\d:\\d\\d)[\\s\\w\\d]*\\]',' inaudible ',x)\n",
    "    # replace email\n",
    "    x = re.sub(r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+|e-mail)\",' email ',x)\n",
    "    \n",
    "    # replace email\n",
    "    x = re.sub(r'[a-zA-Z0-9-]+\\.[a-z]{2,4}',' url ',x)\n",
    "    # replace time\n",
    "    x = re.sub(r'( \\d\\d:\\d\\d| \\d\\d| \\d):\\d\\d( \\w.m.|)',' a.m. ',x)\n",
    "    # replace num\n",
    "    x = re.sub(r' \\d\\.*\\d*[ |,]\\d*',' 7 ',x)\n",
    "    x = re.sub(r' \\d+[\\/|\\:]\\d+',' 7 ',x)\n",
    "    x = re.sub(r'Q&A[|s]',' question-and-answer ',x)\n",
    "    x = re.sub(r'_',' ',x)\n",
    "    x = re.sub(r\"n\\'t\",' nt ',x)\n",
    "    x = re.sub(r\"[\\`\\`|\\'\\']\",'\"',x)\n",
    "    x = re.sub(r\"o\\'clock\",' a.m. ',x)\n",
    "    x = re.sub(r'\"',' ',x)\n",
    "    x = re.sub(r'f\\*\\*\\*','fuck',x)\n",
    "    x = re.sub(r's\\*\\*\\*','shit',x)\n",
    "    return x.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make word to ID lookup table\n",
    "make word embedding matrix from pretrained model based on lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all text in train/test/vali sets and tokenize them with start and end mark\n",
    "# get all words \n",
    "PATH = '/USERS/d8182103/'\n",
    "traindf = pd.read_csv(PATH+'firstimpressionV2/train/text_and_labels.csv')\n",
    "testdf = pd.read_csv(PATH+'firstimpressionV2/test/text_and_labels.csv')\n",
    "validf = pd.read_csv(PATH+'firstimpressionV2/vali/text_and_labels.csv')\n",
    "text = pd.concat([traindf['text'],testdf['text'],validf['text']])\n",
    "all_words = []\n",
    "for i in range(len(text)):\n",
    "    if text.iloc[i] is not np.NaN:\n",
    "        re_text = textfilter(text.iloc[i])\n",
    "        # delete half-words\n",
    "        if '-' in re_text[-5:]:\n",
    "            splits = re_text.split('-')\n",
    "            re_text= '-'.join(splits[:-1])+ ' '+splits[-1]\n",
    "        if '-' in re_text[:5]:\n",
    "            splits = re_text.split('-')\n",
    "            re_text=splits[0] + ' ' +'-'.join(splits[1:])\n",
    "        #remove / and ' = from word\n",
    "        tokens = [re.sub(r\"[\\/|\\'|\\=]\",'',w) for w in word_tokenize(re_text)]\n",
    "        tokens = ['text-start']+tokens+['text-end']\n",
    "        all_words = all_words+tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1traindf = pd.read_csv(PATH+'firstimpressionV1/train/text_and_labels.csv')\n",
    "V2traindf = pd.read_csv(PATH+'firstimpressionV2/train/text_and_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH+'firstimpressionV1/tfrecord/big5/train/mean_std.npy',[np.array(V1traindf.mean(),dtype='float32'),np.array(V1traindf.std(),dtype='float32')])\n",
    "np.save(PATH+'firstimpressionV2/tfrecord/big5/train/mean_std.npy',[np.array(V2traindf.mean(),dtype='float32'),np.array(V2traindf.std(),dtype='float32')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words appearing >= 1 time: 15069\n",
      "#words appearing >= 2 time: 8163\n",
      "#words appearing >= 3 time: 5822\n",
      "#words appearing >= 4 time: 4598\n"
     ]
    }
   ],
   "source": [
    "# count words\n",
    "def find_fre(all_words, n_times):\n",
    "    frequencies = collections.Counter(all_words).most_common()\n",
    "    index = 0\n",
    "    for f in frequencies:\n",
    "        if f[1] == n_times:\n",
    "            return index\n",
    "        else:\n",
    "            index +=1\n",
    "    unique_words=len(frequencies)\n",
    "    return unique_words\n",
    "print(\"#words appearing >= 1 time: %d\" %find_fre(all_words, 0))\n",
    "print(\"#words appearing >= 2 time: %d\" %find_fre(all_words, 1))\n",
    "print(\"#words appearing >= 3 time: %d\" %find_fre(all_words, 2))\n",
    "print(\"#words appearing >= 4 time: %d\" %find_fre(all_words, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j.c.\n",
      "multiplayers\n",
      "darkstalkers\n",
      "100k-ish\n",
      "brizzell\n",
      "togashi\n",
      "mari2\n",
      "fanta-sci\n",
      "micro-focus\n",
      "macklemore\n",
      "sheeran\n",
      "morgans\n",
      "mervin\n",
      "elgato\n",
      "hd60\n",
      "never-world\n",
      "usopp\n",
      "suaves\n",
      "arpock\n",
      "arock\n",
      "capitillo\n",
      "noselipseptum\n",
      "jethro\n",
      "asmr\n",
      "jannie\n",
      "rga\n",
      "charizard\n",
      "charmander\n",
      "patreon\n",
      "anotonio\n",
      "caden\n",
      "phasma\n",
      "twittie\n",
      "twitties\n",
      "dychesfamq\n",
      "dychesfamqanda\n",
      "shulman\n",
      "yanni\n",
      "rimmel\n",
      "stimu\n",
      "altoids\n",
      "malloch\n",
      "correctionals\n",
      "amttg\n",
      "koll\n",
      "slgs\n",
      "dykster\n",
      "monitization\n",
      "pre-shoot\n",
      "jay-man\n",
      "eriktv365\n",
      "dipbrow\n",
      "ab-work\n",
      "graceffa\n",
      "blaha\n",
      "wheeder\n",
      "bradwest\n",
      "doncamatic\n",
      "gorillaz\n",
      "officialfirephoenix\n",
      "leena\n",
      "justkissmyfrog\n",
      "beautycon\n",
      "yiannopoulos\n",
      "garaholic\n",
      "csas\n",
      "rkt\n",
      "gmos\n",
      "geo-cashing\n",
      "bellingham\n",
      "motherxteresa\n",
      "ophelia\n",
      "lovibond\n",
      "leafgreen\n",
      "iffyier\n",
      "am-raps\n",
      "auto-regulates\n",
      "am-rap\n",
      "mommyhood\n",
      "berneseoftherockies\n",
      "kayden\n",
      "post-christmas\n",
      "freesa\n",
      "therese\n",
      "eviegilson\n",
      "vlogmas\n",
      "manhasset\n",
      "garnier\n",
      "volumizer\n",
      "mind-muscle\n",
      "cp9\n",
      "mattshea\n",
      "jacksepticeye\n",
      "situationship\n",
      "omiyage\n",
      "hanabi\n",
      "kyushu\n",
      "canon-g7x\n",
      "aeropostale\n",
      "650d\n",
      "tectronic\n",
      "mark-6\n",
      "retroarch\n",
      "non-jailbroken\n",
      "degarmo\n",
      "rbf\n",
      "lanab4eva\n",
      "eight-ish\n",
      "deju\n",
      "busan\n",
      "dscwx300\n",
      "erimophilas\n",
      "ficinia\n",
      "scaevola\n",
      "waterized\n",
      "rapunzel\n",
      "bvlgari\n",
      "-9:30pm\n",
      "pm-10:30pm\n",
      "emite\n",
      "heart-based\n",
      "marsoness\n",
      "jered\n",
      "fricko\n",
      "schelen\n",
      "108-inch\n",
      "waluigi\n",
      "mckelvey\n",
      "swiss-german\n",
      "joby\n",
      "griptight\n",
      "gorillapod\n",
      "experie\n",
      "leanne\n",
      "daling\n",
      "screenshotting\n",
      "dorito\n",
      "didact\n",
      "missiledinenlin\n",
      "missiledine\n",
      "ngt\n",
      "sephora\n",
      "vays\n",
      "boughten\n",
      "blogness\n",
      "kunis\n",
      "brownsburrow\n",
      "anarchistcommunist\n",
      "duck-sized\n",
      "goombas\n",
      "deliv\n",
      "meijer\n",
      "copshop\n",
      "dykstra\n",
      "luffy\n",
      "trafficky\n",
      "emelio\n",
      "joico\n",
      "biolage\n",
      "vdr\n",
      "ba-mazing\n",
      "r.b.i\n",
      "echovox\n",
      "craislist\n",
      "lawley\n",
      "book-mas\n",
      "girardot\n",
      "shanique\n",
      "kill-wise\n",
      "rookie..\n",
      "p-s-i-r\n",
      "3mj\n",
      "burdo\n",
      "lumas\n",
      "tobbit\n",
      "east-indian\n",
      "esau\n",
      "sirus\n",
      "hegal\n",
      "murica\n",
      "wyc\n",
      "they..\n",
      "river35\n",
      "pinner\n",
      "certai\n",
      "vgc\n",
      "lortab\n",
      "scorcese\n",
      "coderbiller\n",
      "l.a..\n",
      "comment-happy\n",
      "kohnnie\n",
      "johlex\n",
      "loompas\n",
      "tove\n",
      "positraining\n",
      "martinez1031\n",
      "jerbie\n",
      "akali\n",
      "megacon\n",
      "wii-u\n",
      "gigastorm\n",
      "sweezy\n",
      "friend-wise\n",
      "ultron\n",
      "40+-year-old\n",
      "salbaza\n",
      "uh-\n",
      "luten\n",
      "botticelli\n",
      "boxycharm\n",
      "rpe\n",
      "amraps\n",
      "christmastime\n",
      "hazleton\n",
      "whatever..do\n",
      "eeve\n",
      "z87ha\n",
      "lip-smack\n",
      "coolerized\n",
      "albie\n",
      "darryl\n",
      "imovie\n",
      "gese\n",
      "jk18\n",
      "wiclef\n",
      "revlon\n",
      "glitterly\n",
      "glitteraly\n",
      "glittle\n",
      "asmeal\n",
      "loius\n",
      "vuitton\n",
      "wme\n",
      "youtube-\n",
      "non-seven\n",
      "simplynessa\n",
      "alyssaforever\n",
      "cannage\n",
      "enamic\n",
      "cerec\n",
      "ucf\n",
      "mirfandas\n",
      "dbof\n",
      "center-parks\n",
      "homefries\n",
      "equestria\n",
      "mcdon\n",
      "stylebysav\n",
      "azkaban\n",
      "213.\n",
      "arianna\n",
      "face-cams\n",
      "vanderberg\n",
      "milani\n",
      "o2l\n",
      "lokf\n",
      "advi\n",
      "skylar\n",
      "shakeology\n",
      "dimacio\n",
      "face-cam\n",
      "nerv\n",
      "mine-imator\n",
      "absolutely-\n",
      "jeanine\n",
      "thumbsing\n",
      "yu-gi\n",
      "multita-\n",
      "349s\n",
      "tacs\n",
      "wallgreens\n",
      "over-clocking\n",
      "devinsupertramp\n",
      "pottorff\n",
      "aleah\n",
      "mathis\n",
      "askmathis\n",
      "urrr\n",
      "tri-force\n",
      "up-size\n",
      "descri-\n",
      "festive-y\n",
      "ruml\n",
      "brittanyruml\n",
      "fassbender\n",
      "antelope13\n",
      "fleek\n",
      "antone\n",
      "spruch\n",
      "pixiwoo\n",
      "mcjuggernuggets\n",
      "jayden\n",
      "ameil\n",
      "necram\n",
      "mumin\n",
      "valentines-wilkins\n",
      "nordstrom\n",
      "cuber\n",
      "ivs\n",
      "hiit\n",
      "ipod-\n",
      "extre\n",
      "oreal\n",
      "fraps\n",
      "bandicamp\n",
      "dead-animal-based\n",
      "whole-food-plant-based\n",
      "spartachris\n",
      "blackandfishslash\n",
      "dumali\n",
      "ninja-ninja\n",
      "monk-monk\n",
      "hughey\n",
      "eyebr\n",
      "kilimanjaro\n",
      "toubkal\n",
      "conje\n",
      "animeanima\n",
      "android-ware\n",
      "yuna\n",
      "rdi\n",
      "xxantony619xx\n",
      "otg\n",
      "i-can-barely-bench-my-body-weight-pecks\n",
      "seeley\n",
      "deena\n",
      "midnighters\n",
      "resolvement\n",
      "choosing-\n",
      "scaves\n",
      "hdo\n",
      "poshmark\n",
      "pewtiepie\n",
      "rim-tone\n",
      "portr\n",
      "atlassian\n",
      "dick-sized\n",
      "snapchats\n",
      "asked-\n",
      "noakhali\n",
      "dahanu\n",
      "dhakar\n",
      "raykovic\n",
      "sloggy\n",
      "canelo\n",
      "victo-\n",
      "pewdy\n",
      "constanttk\n",
      "constanttweets\n",
      "taily\n",
      "infj\n",
      "biddeford\n",
      "and..\n",
      "girltalkwhail\n",
      "dlg\n",
      "chalkit222\n",
      "adju\n",
      "dremeling\n",
      "v-trigger\n",
      "v-reversal\n",
      "eye-lashes\n",
      "halfkpartay\n",
      "zaber\n",
      "kariya\n",
      "thatbullpencatcher8\n",
      "arby\n",
      "chik-fil-a\n",
      "alabasterslim\n",
      "kevl\n",
      "millman\n",
      "warri\n",
      "jordanlastkings\n",
      "6sr\n",
      "deltech\n",
      "relegat\n",
      "grandmommy\n",
      "kohler\n",
      "phasonator\n",
      "cmg\n",
      "dixiecrats\n",
      "mccarpet\n",
      "stick-men\n",
      "zenthar\n",
      "tekna\n",
      "npnas\n",
      "luthy\n",
      "sadde\n",
      "t-e-k\n",
      "daegu\n",
      "gyeongbuk\n",
      "chinwe\n",
      "leann\n",
      "truven\n",
      "christmas-y\n",
      "beyoncé\n",
      "georgey\n",
      "summersbeautyxx\n",
      "outdoors-y\n",
      "tangus\n",
      "spreadshirt\n",
      "chrishelle\n",
      "neverfull\n",
      "a.e.i.o.u\n",
      "tufe\n",
      "ashlynn\n",
      "compli\n",
      "asklokf\n",
      "ignaceous\n",
      "furbies\n",
      "silverreploid\n",
      "things..\n",
      "spicatto\n",
      "marik\n",
      "like-dislike\n",
      "angelovp\n",
      "badcom\n",
      "blazegaming21\n",
      "troye\n",
      "balmain\n",
      "tpg\n",
      "gabinetto\n",
      "static-y\n",
      "watch..\n",
      "neutrogena\n",
      "burrough\n",
      "mattify\n",
      "nsaid\n",
      "sea-biscuit\n",
      "messengering\n",
      "wide-grip\n",
      "close-gripmedium-grip\n",
      "pming\n",
      "siddian\n",
      "nebs\n",
      "gerand\n",
      "cyborg-centered\n",
      "cayden\n",
      "flavorous\n",
      "yukota\n",
      "gmm\n",
      "culley\n",
      "philippians\n",
      "mainreality\n",
      "youtube-ers\n",
      "exodusface\n",
      "alie\n",
      "volvic\n",
      "ribena\n",
      "guspe\n",
      "haleyrosemusic\n",
      "you..you\n",
      "for..\n",
      "-nine:30\n",
      "jo-ju-jo-ju-ju\n",
      "hour-swap\n",
      "spiralizing\n",
      "inspiralize\n",
      "condren\n",
      "trulytrucly\n",
      "godiva\n",
      "youtube-verse\n",
      "extensions-\n",
      "gansben\n",
      "deshard\n",
      "concept-wise\n",
      "rahasthan\n",
      "foucci\n",
      "brimbles\n",
      "kenna\n",
      "dwd\n",
      "panera\n",
      "salvadi\n",
      "soulcalibur\n",
      "re-film\n",
      "thank-\n",
      "descri\n",
      "unattracted\n",
      "rx-100\n",
      "vscocam\n",
      "goanimate\n",
      "iguazu\n",
      "florianopolis\n",
      "guys..\n",
      "collosus\n",
      "fnaf\n",
      "miata\n",
      "light..\n",
      "iracing\n",
      "deluca\n",
      "uplikes\n",
      "winter-ish\n",
      "minatsuki-chan\n",
      "2ds\n",
      "yama-com\n",
      "howes\n",
      "moviemovies\n",
      "kaylee\n",
      "kay-girl\n",
      "kay-kay\n",
      "boysandpizza\n",
      "478.\n",
      "zinnie\n",
      "\n",
      "skyflygirl\n",
      "declates\n",
      "ierodun\n",
      "albrecht\n",
      "starbound\n",
      "bibbity-bobbity-boop\n",
      "qna17\n",
      "motorplex\n",
      "roderick\n",
      "housewifes\n",
      "accl\n",
      "mcgraw\n",
      "edinboro\n",
      "frodo\n",
      "non-prep\n",
      "senuros\n",
      "adrianna\n",
      "prestiged\n",
      "yoralphy\n",
      "y-o-r-a-l-p-h-y\n",
      "epcot\n",
      "rawls\n",
      "kuroko\n",
      "vsco\n",
      "bonham\n",
      "purple-y\n",
      "markels\n",
      "e324t\n",
      "bacintratus\n",
      "descreptive\n",
      "b-b-b-b-b-boy\n",
      "alandlg\n",
      "annabelle\n",
      "bugeye\n",
      "flip-through\n",
      "nayelli\n",
      "nymphette\n",
      "stemp\n",
      "whalish\n",
      "or..\n",
      "muffinstumps40qna\n",
      "nakeya\n",
      "echoplex\n",
      "fruska\n",
      "ba-bam\n",
      "vidcon\n",
      "eeu\n",
      "over-saturate\n",
      "up-voted\n",
      "esmerel\n",
      ".tm\n",
      "zoeva\n",
      "gts6\n",
      "mgks\n",
      "tuffy\n",
      "showsseries\n",
      "a+\n",
      "mattefies\n",
      "sapp\n",
      "in-eye\n",
      "flickson\n",
      "2k16\n",
      "hakluyt\n",
      "cacatsavoy\n",
      "hetras\n",
      "r563\n",
      "kattpacalypse\n",
      "fast-tech\n",
      "vloggity\n",
      "pumba\n",
      "meeko\n",
      "pocahontas\n",
      "jemima\n",
      "qts\n",
      "rgh\n",
      "singerband\n",
      "delonge\n",
      "nordstroms\n",
      "vegan-ish\n",
      "kyros\n",
      "customly\n",
      "imperfecttap\n",
      "dicaprio\n",
      "caitlyn\n",
      "macbooks\n",
      "300-2000\n",
      "fiveish\n",
      "appre\n",
      "estevez\n",
      "urbay\n",
      "cambria\n",
      "boxster\n",
      "agamemnon\n",
      "oresteia\n",
      "ambergervadexo\n",
      "enneagrams\n",
      "itssistersyd\n",
      "acx\n",
      "constantinides\n",
      "c.t\n",
      "usiff\n",
      "matthysse\n",
      "ruslan\n",
      "provodnikov\n",
      "wildfox\n",
      "cis-man\n",
      "flashback-\n",
      "contam\n",
      "brosnan\n",
      "kierkegaard\n",
      "strasbourg\n",
      "c.s\n",
      "keyosha\n",
      "dkny39\n",
      "tattooer\n",
      "ndhn\n",
      "kierakta\n",
      "coby\n",
      "gorillavid\n",
      "a-whole-nother\n",
      "uw6921\n",
      "goulding\n",
      "zoop\n",
      "micing\n",
      "zaxby\n",
      "christful\n",
      "chiefzod\n",
      "zip-drives\n",
      "zip-discs\n",
      "zip-things\n",
      "likedislike\n",
      "nars\n",
      "transition-y\n",
      "honky-dory\n",
      "blackenedfish\n",
      "twitterblackenedfish\n",
      "disappoin\n",
      "facebookdarrellfitness\n",
      "you-tubers\n",
      "self-dependencies\n",
      "muscles..\n",
      "much..\n",
      "self-assure\n",
      "mincraft\n",
      "ne-uh\n",
      "neegan\n",
      "petey\n",
      "farrah\n",
      "majia\n",
      "slyke\n",
      "akg\n",
      "coven-less\n",
      "maybelline\n",
      "mrs.wood\n",
      "8:15.\n",
      "hinkler\n",
      "levitt\n",
      "rosarie\n",
      "downbar\n",
      "shelia\n",
      "vieng\n",
      "evie\n",
      "copperfield\n",
      "insur\n",
      "asamar\n",
      "1-5-0-6\n",
      "israkhan\n",
      "israkhan3\n",
      "elkaim\n",
      "broher\n",
      "exenta\n",
      "technomodel\n",
      "r.r\n",
      "picking-\n",
      "google+\n",
      "singingbunny\n",
      "bit..\n",
      "roomtube\n",
      "nostrand\n",
      "askelsa\n",
      "ambra\n",
      "auria\n",
      "mitza\n",
      "wickliff\n",
      "gt5\n",
      "shneider\n",
      "soupology\n",
      "lp4112\n",
      "tallahassee\n",
      "beauty-ish\n",
      "mcclain\n",
      "iju\n",
      "stravaganza\n",
      "h-e\n",
      "clan-mate\n",
      "snv\n",
      "showsmovies\n",
      "isleshanalluri\n",
      "die-cuts\n",
      "koby\n",
      "christinalegend\n",
      "xypand15\n",
      "pericoping\n",
      "iomegirl\n",
      "icarl\n",
      "o-to-l\n",
      "uplif\n",
      "alyssia\n",
      "hoo-\n",
      "breathing-\n",
      "umei\n",
      "englishes\n",
      "sandersongirl\n",
      "felixmartinsson\n",
      "fitlife\n",
      "milenko\n",
      "itps\n",
      "mariam\n",
      "u.a.e\n",
      "talio\n",
      "j45\n",
      "kellory\n",
      "kental\n",
      "coleco\n",
      "urks\n",
      "ec-1118\n",
      "pubs-\n",
      "drunk-\n",
      "crickity\n",
      "dead-lifts\n",
      "exercising-\n",
      "holdernessvideos\n",
      "pay-tron\n",
      "l.a\n",
      "ballet-linked\n",
      "thenewcree\n",
      "ansel\n",
      "elgort\n",
      "rhianna\n",
      "non-youtube\n",
      "halley\n",
      "probably-\n",
      "electro-swing\n",
      "mkl\n",
      "kelis\n",
      "anyhew\n",
      "kleg\n",
      "forg\n",
      "anaheim\n",
      "blizzcon\n",
      "ps2s\n",
      "hollister\n",
      "jaden\n",
      "headcanon\n",
      "jobdream\n",
      "novaowlctc\n",
      "j.k\n",
      "eccleston\n",
      "nose-piercing\n",
      "kaitlyn\n",
      "pretty-\n",
      "kitama\n",
      "wfg\n",
      "tarbucklecurl\n",
      "limpo\n",
      "limpy\n",
      "blimpo\n",
      "kurasawa\n",
      "mccall\n",
      "hemsworth\n",
      "pre-uploaded\n",
      "imats\n",
      "annik\n",
      "lip-liner\n",
      "caspar\n",
      "quincey\n",
      "9-80.\n",
      "25-10-15-20\n",
      "knope\n",
      "galentine\n",
      "braid-outs\n",
      "subsc\n",
      "zanthee\n",
      "collabing\n",
      "kalen\n",
      "geysler\n",
      "transactionary\n",
      "2k10\n",
      "coltrane\n",
      "ravvy\n",
      "tyner\n",
      "barn-\n",
      "cousins-\n",
      "negan\n",
      "hyperdimension\n",
      "neptunia\n",
      "zloken\n",
      "brahimovich\n",
      "flonk\n",
      "crayolas\n",
      "yomicon\n",
      "ceuticals\n",
      "l-ascorbic\n",
      "rosita\n",
      "as10\n",
      "sexymind\n",
      "j.l\n",
      "cashore\n",
      "graceling\n",
      "manapul\n",
      "madikwe\n",
      "nugin\n",
      "360.\n",
      "star-myu\n",
      "nikkietutorials\n",
      "jaclyn\n",
      "hill-\n",
      "arnell\n",
      "mris\n",
      "methainide\n",
      "citrus-y\n",
      "a-gar-io\n",
      "agario\n",
      "tidwell\n",
      "scem\n",
      "baylor\n",
      "originalminion\n",
      "g-eazy\n",
      "glog\n",
      "medium-action\n",
      "medsurg\n",
      "ifbb\n",
      "farland\n",
      "legitly\n",
      "spesssshhhh\n",
      "catwalk-\n",
      "pve\n",
      "hermès\n",
      "birkin\n",
      "uriah\n",
      "dolarhyde\n",
      "paolini\n",
      "eragon\n",
      "verlander\n",
      "sunshineprincess\n",
      "facebookbringbackthelosangelesraiders\n",
      "samoans\n",
      "mac-compatible\n",
      "panzy\n",
      "greyhoundwill\n",
      "phers\n",
      "cteph\n",
      "j-c-s\n",
      "multiple-video\n",
      "poppins\n",
      "otp\n",
      "gretz\n",
      "g.i\n",
      "xbox1\n",
      "montel\n",
      "yann\n",
      "segwaying\n",
      "tyrelis\n",
      "stephanies1n3\n",
      "xjames\n",
      "xjawz\n",
      "nuthead\n",
      "brittneysadvice\n",
      "pnw\n",
      "rampage887\n",
      "museffect\n",
      "so-so-so-so\n",
      "..to\n",
      "unbear\n",
      "meyri\n",
      "jacobfreshpvp\n",
      "supdaily\n",
      "booktube\n",
      "neo-economics\n",
      "fritos\n",
      "seeds-\n",
      "asmrrequests\n",
      "filmora\n",
      "cortana\n",
      "haddaway\n",
      "5200k\n",
      "glozell\n",
      "steve-o\n",
      "hiv-\n",
      "yander\n",
      "holdernessvide\n",
      "meaning-full\n",
      "anish-\n",
      "gizelle\n",
      "gi-gi\n",
      "periscoping\n",
      "xypn15\n",
      "molnar\n",
      "m-o-l-n-a-r\n",
      "colaboaze\n",
      "ocds\n",
      "gordmans\n",
      "transmissional\n",
      "over-hip\n",
      "o-m-g\n",
      "renwood\n",
      "spellman\n",
      "joella\n",
      "berta\n",
      "berty\n",
      "bernese\n",
      "de-wormer\n",
      "gamergirldani\n",
      "mithibai\n",
      "toshasadi\n",
      "hurein\n",
      "mysticblazer\n",
      "bro-\n",
      "younow\n",
      "pre-clipped\n",
      "zire\n",
      "ay-tah-uh\n",
      "swag-way\n",
      "story-board\n",
      "lovestest\n",
      "cozi\n",
      "c-o-z-i\n",
      "askstephanie\n",
      "inkadinkado\n",
      "vintagie\n",
      "karoko\n",
      "bitcon\n",
      "thomas-\n",
      "hilah\n",
      "60minute\n",
      "comes-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adt\n",
      "marrakesh\n",
      "ctaa\n",
      "marth\n",
      "hyperness\n",
      "aquabogging\n",
      "amazo\n",
      "brethrens\n",
      "chit-chatty\n",
      "stonie\n",
      "expand-there\n",
      "go-ah\n",
      "super-sarcastic\n",
      "petsmart\n",
      "mquan\n",
      "ddzeets\n",
      "colley\n",
      "estée\n",
      "suvcrossover\n",
      "giarda\n",
      "essie\n",
      "chinchilly\n",
      "2007-08.\n",
      "g-girls.jpeg\n",
      "cheez-its\n",
      "dreyfuss\n",
      "50¢\n",
      "alanah\n",
      "a-l-a-n-a-h-f-c\n",
      "alanahfc\n",
      "gtas\n",
      "haymitch\n",
      "mini-bags\n",
      "slowly-\n",
      "michonne\n",
      "bosluna\n",
      "odx\n",
      "koreatown\n",
      "super-attracted\n",
      "camp-y\n",
      "those..\n",
      "lindt\n",
      "everyb\n",
      "suesub\n",
      "fransisco\n",
      "musicartist\n",
      "doregama1\n",
      "ifunny\n",
      "checkvile\n",
      "hasbro\n",
      "starkiller\n",
      "izombie\n",
      "sandburg\n",
      "2k11\n",
      "phifer\n",
      "hoursday\n",
      "the..\n",
      "g99\n",
      "jean-martin\n",
      "fortier\n",
      "myhalosgood\n",
      "nosscr\n",
      "favorite-\n",
      "dlp\n",
      "dikes-\n",
      "c-curl\n",
      "ganjo\n",
      "mm-hmm\n",
      "b17\n",
      "mega-con\n",
      "slush-puppies\n",
      "fluttershy\n",
      "storm-well\n",
      "thing-but\n",
      "askmathison\n",
      "battlespot\n",
      "mikinlvoe\n",
      "m-i-k-i-n-l-v-o-e\n",
      "sydneyppatterson\n",
      "toule\n",
      "kindofdry\n",
      "tunesia\n",
      "pro-lactin\n",
      "youtube-er\n",
      "skyfly\n",
      "sum2015\n",
      "keagan\n",
      "beason\n",
      "infernape\n",
      "sceptile\n",
      "blaney\n",
      "home-schooler\n",
      "stormstrong\n",
      "cmking03\n",
      "blondest\n",
      "detail..\n",
      "binondo\n",
      "step-nephew\n",
      "beauty-related\n",
      "widdled\n",
      "mid-trip\n",
      "duresta\n",
      "pacuto\n",
      "jehoshaphat\n",
      "patrons-only\n",
      "because..\n",
      "j.k.\n",
      "gross-\n",
      "longas\n",
      "guin\n",
      "swim-two-birds\n",
      "misterwives\n",
      "pvris\n",
      "p-v-r-i\n",
      "e-juices\n",
      ":42\n",
      "school-that\n",
      "photogrpahy\n",
      "qe2\n",
      "branching-out\n",
      "weird-ish\n",
      "askjackfruit\n",
      "medifast\n",
      "ryback\n",
      "reemy\n",
      "prettywitty\n",
      "inez\n",
      "inez1\n",
      "inezl\n",
      "co-workspace\n",
      "support.t\n",
      "markiplier\n",
      "w.w.\n",
      "modelingmimicking\n",
      "easi\n",
      "micro-bits\n",
      "ottowa\n",
      "glaad\n",
      "girling\n",
      "shakras\n",
      "ouji\n",
      "berkin\n",
      "fasttech\n",
      "ytp\n",
      "unorganizedness\n",
      "cats-\n",
      "camcrunch\n",
      "dailypolina\n",
      "aimdream\n",
      "alkalizes\n",
      "berto\n",
      "waverly\n",
      "meghan\n",
      "trainor\n",
      "leonah88\n",
      "anelle\n",
      "video..\n",
      "susub\n",
      "pyroid\n",
      "foundation-wise\n",
      "banwell\n",
      "youtubeing\n",
      "tshs\n",
      "pths\n",
      "fshs\n",
      "pv3\n",
      "give..\n",
      "kingsman\n",
      "colourpop\n",
      "qx4\n",
      "is250\n",
      "jaimie\n",
      "kight\n",
      "itaewon\n",
      "nouncy\n",
      "under-bite\n",
      "rusev\n",
      "olaplex\n",
      "annotat\n",
      "pretty..\n",
      "uniquenesses\n",
      "x-games\n",
      "constructively-\n",
      "chomper\n",
      "slitherin\n",
      "hufflepuff\n",
      "curlique\n",
      "saccone\n",
      "joly\n",
      "erauschitav\n",
      "nohomo\n",
      "iamshmoove\n",
      "timey-whimey\n",
      "grisham\n",
      "cimarron\n",
      "o-case\n",
      "editorsinspirations\n",
      "mezusa\n",
      "airline-\n",
      "indesolu\n",
      "so-cal\n",
      "noticed-\n",
      "14-sh\n",
      "soroles\n",
      "protheif\n",
      "a-e-i-o-u\n",
      "3.6.\n",
      "eotgg\n",
      "m.m.m\n",
      "cici\n",
      "pack-it\n",
      "her..\n",
      "were-they\n",
      "shartimus\n",
      "bamf\n",
      "mafex\n",
      "mcfarland\n",
      "maurie\n",
      "youtubing\n",
      "pocker\n",
      "jazzyxthree\n",
      "grecian\n",
      "dirpy\n",
      "avc\n",
      "gamegame\n",
      "mopps\n",
      "paper..\n",
      "mkx\n",
      "lovecraft\n",
      "badescu\n",
      "emolliating\n",
      "algenist\n",
      "searcy\n",
      "stroman\n",
      "weirdish\n",
      "geistler\n",
      "salib\n",
      "christmasy\n",
      "undertale\n",
      "brennen\n",
      "jocelyn\n",
      "guardado\n",
      "ungerer\n",
      "sergion\n",
      "re-renting\n",
      "just-i\n",
      "chandelure\n",
      "tropius\n",
      "combofiend\n",
      "enchikari\n",
      "nevil\n",
      "time-passer\n",
      "thundermist\n",
      "boatless\n",
      "anyway..\n",
      "fmri\n",
      "club-inclined\n",
      "estee\n",
      "timon\n",
      "pumbaa\n",
      "eggo\n",
      "truvia\n",
      "lemonly\n",
      "npcs\n",
      "hayleyquinn\n",
      "youmail\n",
      "s..\n",
      "abilify\n",
      "rexulti\n",
      "chevon\n",
      "trafalgar\n",
      "smartgirls\n",
      "philadel..\n",
      "four-wood\n",
      "veganize\n",
      "non-marvel\n",
      "non-dc\n",
      "tim..\n",
      "cjpforprez\n",
      "recor\n",
      "petshops\n",
      "hdchs9\n",
      "mcn\n",
      "eva-01\n",
      "eva-02\n",
      "pinchy\n",
      "acheyness\n",
      "skalia\n",
      "tri-pod\n",
      "asklauren\n",
      "super-picky\n",
      "kenobi\n",
      "obi-wan\n",
      "co-lab\n",
      "i-with\n",
      "stuff-if\n",
      "upass\n",
      "bellagio\n",
      "gammer\n",
      "nsx\n",
      "macaulay\n",
      "culkin\n",
      "loveland\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13826"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count words and check whether they are OOV\n",
    "# delete words that are OOV\n",
    "frequencies = collections.Counter(all_words)\n",
    "keys = [key for key in frequencies]\n",
    "for key in keys:\n",
    "    try:\n",
    "        model.most_similar(key)\n",
    "    except KeyError:\n",
    "        if key !='text-end' and key !='text-start':\n",
    "            print(key)\n",
    "            frequencies.pop(key)\n",
    "len(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save words in dataframe, sort and save the file.\n",
    "keys = [key for key in frequencies]\n",
    "values = [frequencies[key] for key in keys]\n",
    "embedding = []\n",
    "for key in keys:\n",
    "    if key=='text-start' or key=='text-end' :\n",
    "        embedding.append(model['-'])\n",
    "    else:\n",
    "        embedding.append(model[key])\n",
    "        \n",
    "df = pd.DataFrame({'word':keys,'count':values, 'vector':embedding},columns=['word','count','vector'])\n",
    "newdf=df.sort_values('count',ascending=0).reset_index(drop=1)\n",
    "\n",
    "#save word to ID map\n",
    "# PATH = '/USERS/d8182103/'\n",
    "with open(PATH+'firstimpressionV2/tfrecord/text/word2ID.txt','w') as f:\n",
    "    for w in newdf['word']:\n",
    "        f.write(w+'\\n')\n",
    "    # padding word\n",
    "    f.write('padding-mark'+'\\n')\n",
    "# save word embedding matrix\n",
    "np.save(PATH+'firstimpressionV2/tfrecord/text/embedding_matrix',np.vstack(newdf['vector']))\n",
    "# save UNK word vector\n",
    "np.save(PATH+'firstimpressionV2/tfrecord/text/UNK',np.vstack(model['UNK']).reshape(1,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# put SOS and SOE at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/USERS/d8182103/'\n",
    "embedding_matrix = np.load(PATH+'firstimpressionV2/tfrecord/text/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_SOE = np.array(embedding_matrix[7:9], copy=True)\n",
    "firsttwo = np.array(embedding_matrix[0:2], copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00026366691"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsttwo.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0022383332"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_SOE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00026366691"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[7:9].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0022383332"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0:2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[7:9] = firsttwo\n",
    "embedding_matrix[0:2] = SOS_SOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH+'firstimpressionV2/tfrecord/text/embedding_matrix',embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save text training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels and text(if it has)\n",
    "def savetext(setname, dataset='firstimpressionV2', modality='text',filename = 'text_and_labels.csv', PATH = '/USERS/d8182103'):\n",
    "    csvpath = PATH+'/%s/%s/%s' %(dataset, setname, filename)\n",
    "    df = pd.read_csv(csvpath)\n",
    "    print('open csv: %s' %csvpath)\n",
    "    textpath = PATH+'/%s/tfrecord/%s/%s/%s.txt' %(dataset, modality, setname, setname)\n",
    "    print('save txt: %s' %textpath)\n",
    "    fw=open(textpath.split('.txt')[0]+'_videos_without_transcription.txt','w')\n",
    "    print('save txt: %s' %(textpath.split('.txt')[0]+'_videos_without_transcription.txt'))\n",
    "    with open(textpath,'w') as f:\n",
    "        for i in range(len(df)):\n",
    "            # text pre-processing\n",
    "            # make sure it has transcription\n",
    "            if type(df.loc[i,'text']) is not type('s'):\n",
    "                fw.write(df.loc[i,'video']+'\\n')\n",
    "                print(df.loc[i,'text'],type(df.loc[i,'text']),df.loc[i,'video'])\n",
    "            else:\n",
    "                # filter text\n",
    "                re_text = textfilter(df.loc[i,'text'])\n",
    "                # delete half-words\n",
    "                if '-' in re_text[-5:]:\n",
    "                    splits = re_text.split('-')\n",
    "                    re_text= '-'.join(splits[:-1])+ ' '+splits[-1]\n",
    "                if '-' in re_text[:5]:\n",
    "                    splits = re_text.split('-')\n",
    "                    re_text=splits[0] + ' ' +'-'.join(splits[1:])\n",
    "                # tokenize text\n",
    "                #remove / and ' from word\n",
    "                tokens = [re.sub(r\"[\\/|\\'|\\=]\",'',w) for w in word_tokenize(re_text)]\n",
    "                tokens = ' '.join(tokens)\n",
    "                # add start and set mark\n",
    "                tokens = 'text-start '+tokens+' text-end\\n'\n",
    "                f.write(tokens)\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open csv: /USERS/d8182103/firstimpressionV2/train/text_and_labels.csv\n",
      "save txt: /USERS/d8182103/firstimpressionV2/tfrecord/text/train/train.txt\n",
      "save txt: /USERS/d8182103/firstimpressionV2/tfrecord/text/train/train_videos_without_transcription.txt\n",
      "nan <class 'float'> iYVJt41_q7M.002.mp4\n",
      "nan <class 'float'> 4LZJvOecyM8.005.mp4\n",
      "nan <class 'float'> YC3X1DcnUrk.000.mp4\n",
      "nan <class 'float'> ztyBhnjtrz0.000.mp4\n",
      "nan <class 'float'> JTmq4k4uQCY.003.mp4\n",
      "nan <class 'float'> KRo-x2uoHUg.003.mp4\n",
      "nan <class 'float'> HhC2cGFFZeY.000.mp4\n",
      "nan <class 'float'> cRDYrvxRJ6U.001.mp4\n",
      "open csv: /USERS/d8182103/firstimpressionV2/test/text_and_labels.csv\n",
      "save txt: /USERS/d8182103/firstimpressionV2/tfrecord/text/test/test.txt\n",
      "save txt: /USERS/d8182103/firstimpressionV2/tfrecord/text/test/test_videos_without_transcription.txt\n",
      "nan <class 'float'> JmAQlC-FEV8.000.mp4\n",
      "nan <class 'float'> _plk5k7PBEg.004.mp4\n",
      "nan <class 'float'> L-rmZZP_wj8.005.mp4\n",
      "open csv: /USERS/d8182103/firstimpressionV2/vali/text_and_labels.csv\n",
      "save txt: /USERS/d8182103/firstimpressionV2/tfrecord/text/vali/vali.txt\n",
      "save txt: /USERS/d8182103/firstimpressionV2/tfrecord/text/vali/vali_videos_without_transcription.txt\n"
     ]
    }
   ],
   "source": [
    "for setname in ['train','test','vali']:\n",
    "    savetext(setname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save big5 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tfrecorder import TFrecorder\n",
    "PATH = '/USERS/d8182103/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(df,dataset,setname):\n",
    "    # read file containing the video name that will not be used\n",
    "    delete_list = PATH+('firstimpression%s/tfrecord/text/%s/%s_videos_without_transcription.txt' %(dataset,setname,setname))\n",
    "    if dataset =='V2':\n",
    "        df = df.drop(columns=['text'])\n",
    "    try:\n",
    "        with open(delete_list,'r') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print('firstimpression V1')\n",
    "    else:\n",
    "        # get indexes that should be removed from datafram\n",
    "        remove_indexes = []\n",
    "        for l in lines:\n",
    "            remove_indexes.append(df[df['video']==l[:-1]].index[0])\n",
    "        # drop rows of those indexes\n",
    "        df = df.drop(np.array(remove_indexes))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_big5(dataset,setname):\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(PATH+'firstimpression%s/%s/text_and_labels.csv' %(dataset,setname))\n",
    "    # get correct df\n",
    "    newdf = get_dataframe(df,dataset, setname)\n",
    "    # make df for tfrecord writing\n",
    "    if dataset =='V2':\n",
    "        data_info = pd.DataFrame({'name':['video','big5','job'],\n",
    "                                 'type':['string','float32','float32'],\n",
    "                                 'shape':[(),(5,),()],\n",
    "                                 'isbyte':[True,False,False],\n",
    "                                 \"length_type\":['fixed','fixed','fixed'],\n",
    "                                 \"default\":[np.NaN,np.NaN,np.NaN]})\n",
    "    else:\n",
    "        data_info = pd.DataFrame({'name':['video','big5'],\n",
    "                                 'type':['string','float32'],\n",
    "                                 'shape':[(),(5,)],\n",
    "                                 'isbyte':[True,False],\n",
    "                                 \"length_type\":['fixed','fixed'],\n",
    "                                 \"default\":[np.NaN,np.NaN]})\n",
    "\n",
    "    data_info_path = PATH+'firstimpression%s/tfrecord/big5/data_info.csv' %dataset\n",
    "    data_info.to_csv(data_info_path,index=False)\n",
    "\n",
    "    tfr = TFrecorder()\n",
    "    writer = tf.python_io.TFRecordWriter(PATH+'firstimpression%s/tfrecord/big5/%s/%s.tfrecord' %(dataset,setname,setname))\n",
    "    num_examples = len(newdf)\n",
    "    for i in np.arange(num_examples):\n",
    "        # 要写到tfrecord文件中的字典\n",
    "\n",
    "        features = {}\n",
    "        # get information from dataframe\n",
    "        one_user = newdf.iloc[i]\n",
    "        # write video name\n",
    "        video = one_user['video']\n",
    "        tfr.feature_writer(data_info.iloc[0], video, features)\n",
    "        #print(video)\n",
    "        # write big5\n",
    "        big5 = one_user[['ope', 'con', 'ext', 'agr', 'neu']]\n",
    "        tfr.feature_writer(data_info.iloc[1], big5, features)\n",
    "        #print(big5)\n",
    "        # write job label\n",
    "        if dataset =='V2':\n",
    "            job = one_user['job']\n",
    "            tfr.feature_writer(data_info.iloc[2], job, features)\n",
    "            #print(job)\n",
    "        tf_features = tf.train.Features(feature= features)\n",
    "        tf_example = tf.train.Example(features = tf_features)\n",
    "        tf_serialized = tf_example.SerializeToString()\n",
    "        writer.write(tf_serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstimpression V1\n",
      "V1 train\n",
      "firstimpression V1\n",
      "V1 vali\n",
      "V2 train\n",
      "V2 vali\n",
      "V2 test\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['V1','V2']:\n",
    "    for setname in ['train','vali']:\n",
    "        save_big5(dataset,setname)\n",
    "        print(dataset,setname)\n",
    "save_big5('V2','test')\n",
    "print('V2','test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastText import load_model\n",
    "from tfrecorder import TFrecorder\n",
    "import tensorflow as tf\n",
    "PATH = '/USERS/d8182103/'\n",
    "# 实例化该类\n",
    "tfr = TFrecorder()\n",
    "dataset = 'firstimpressionV2/'\n",
    "setname = 'vali'\n",
    "modalname = 'text'\n",
    "# 当前写的样本数\n",
    "num_examples_per_file = 1000\n",
    "num_so_far = 0\n",
    "PATH = '/USERS/d8182103/'\n",
    "datapath='%s/%s/tfrecord/%s/%s/' %(PATH,dataset,modalname, setname)\n",
    "data_info_path = datapath+'data_info.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastText import load_model\n",
    "from tfrecorder import TFrecorder\n",
    "import tensorflow as tf\n",
    "PATH = '/USERS/d8182103/'\n",
    "tfr = TFrecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_maker(dataset ='V2', setname='train', batchsize =2, shuffle_buffer = 0, epoch =1):\n",
    "    def input_fn():\n",
    "        # big five label dataset\n",
    "        # path\n",
    "        label_info_path = '/USERS/d8182103/firstimpressionV1/tfrecord/big5/data_info.csv'\n",
    "        label_dataset_path = '/USERS/d8182103/firstimpression%s/tfrecord/big5/%s/%s.tfrecord' %(dataset,setname,setname)\n",
    "        # dataset\n",
    "        tfr.get_dataset(paths=[label_dataset_path], data_info=label_info_path)\n",
    "        label_dataset = tfr.dataset_raw\n",
    "\n",
    "        # text dataset\n",
    "        # path\n",
    "        text_dataset_path = '/USERS/d8182103//firstimpression%s/tfrecord/text/%s/%s.txt' %(dataset,setname,setname)\n",
    "        # dataset\n",
    "        text_dataset = tf.data.TextLineDataset(text_dataset_path)\n",
    "        text_dataset = text_dataset.map(lambda x: tf.string_split([x], \" \").values)\n",
    "\n",
    "\n",
    "        # merge\n",
    "        merge_datasets_list = (label_dataset, text_dataset)\n",
    "        merge_dataset = tf.data.Dataset.zip(merge_datasets_list)\n",
    "\n",
    "        # dataset transformation\n",
    "        # shuffle\n",
    "        if shuffle_buffer>10:\n",
    "            merge_dataset = merge_dataset.shuffle(buffer_size=shuffle_buffer)\n",
    "        # batch_padding\n",
    "        padding_info = ({'big5':[None]}, [None])\n",
    "        pad_word = tf.constant('padding-mark')\n",
    "        padding_value = ({'big5':0.0}, pad_word)\n",
    "\n",
    "        merge_dataset = merge_dataset.padded_batch(batch_size, padded_shapes = padding_info, padding_values = padding_value)\n",
    "        merge_dataset = merge_dataset.repeat(epoch)\n",
    "        iterator = merge_dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read dataframe from /USERS/d8182103/firstimpressionV2/tfrecord/big5/train/train.tfrecord x 1\n",
      "   default  isbyte length_type  name shape     type\n",
      "0      NaN   False       fixed  big5   [5]  float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'big5': <tf.Tensor 'IteratorGetNext_6:0' shape=(?, ?) dtype=float32>},\n",
       " <tf.Tensor 'IteratorGetNext_6:1' shape=(?, ?) dtype=string>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset = merge_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'big5': <tf.Tensor 'IteratorGetNext_1:0' shape=(?, ?) dtype=float32>},\n",
       " <tf.Tensor 'IteratorGetNext_1:1' shape=(?, ?) dtype=string>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset = tf.data.Dataset.zip((label_dataset,text_dataset))\n",
    "padding_info = ({'big5':[None],'job':[]},[None])\n",
    "padding_value = ({'big5':0.0,'job':0.0},tf.constant('padding-mark'))\n",
    "pad_dataset = merge_dataset.padded_batch(2, padded_shapes = padding_info,padding_values=padding_value)\n",
    "#.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_merge=pad_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-046704ec5efa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'big5'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msample_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'job'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_text\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "sess.run([sample_label['big5'] , sample_label['job'], sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.6111111 ,  0.64077669,  0.41121495,  0.56043959,  0.625     ],\n",
       "        [ 0.3888889 ,  0.14563107,  0.35514018,  0.46153846,  0.25      ]], dtype=float32),\n",
       " array([[b'text-start', b'maintenance', b',', b'so', b'making', b'sure',\n",
       "         b'that', b'you', b're', b'not', b'letting', b'in', b'drown', b'in',\n",
       "         b'the', b'pan', b',', b'that', b'you', b're', b'cleaning', b'it',\n",
       "         b'our', b'regularly', b',', b'that', b'you', b're', b'using',\n",
       "         b'the', b'pan', b'regularly', b'.', b'those', b'things', b'are',\n",
       "         b'all', b'great', b',', b'but', b'they', b're', b'all', b'a',\n",
       "         b'little', b'higher', b'maintenance', b'that', b'typical',\n",
       "         b'fountain', b'techniques', b'.', b'an', b'ounce', b'of',\n",
       "         b'prevention', b'is', b'worth', b'a', b'pound', b'of', b'cure',\n",
       "         b'.', b'text-end'],\n",
       "        [b'text-start', b'job', b'and', b'depend', b'on', b'you', b'two',\n",
       "         b'for', b'your', b'income', b'when', b'it', b'gets', b'to', b'the',\n",
       "         b'point', b'that', b'it', b's', b'possible', b',', b'yes', b'.',\n",
       "         b'absolutely', b',', b'like', b',', b'absolutely', b'.', b'i',\n",
       "         b'am', b'the', b'type', b'of', b'person', b'that', b'i', b'am',\n",
       "         b'so', b'spontaneous', b'.', b'text-end', b'padding-mark',\n",
       "         b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "         b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "         b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "         b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "         b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "         b'padding-mark']], dtype=object)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([merge_dataset[0]['big5'] , merge_dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.54444444,  0.44660193,  0.55140185,  0.49450549,  0.46875   ],\n",
       "        [ 0.62222224,  0.80582523,  0.48598132,  0.78021979,  0.72916669]], dtype=float32),\n",
       " array([ 0.48598132,  0.62616825], dtype=float32),\n",
       " array([[b'text-start', b'but', b'i', b'ca', b'nt', b'say', b'i', b've',\n",
       "         b'kept', b'count', b'of', b'how', b'many', b'times', b'i', b'put',\n",
       "         b'my', b'finger', b'in', b'the', b'oven', b'.', b'sorry', b'.',\n",
       "         b'what', b's', b'the', b'color', b'of', b'the', b'ocean', b'?',\n",
       "         b'the', b'ocean', b'is', b'actually', b'bright', b'orange', b',',\n",
       "         b'and', b'if', b'you', b'see', b'it', b',', b'it', b's', b'blue',\n",
       "         b',', b'you', b're', b'actually', b'...', b'text-end'],\n",
       "        [b'text-start', b'because', b'as', b'again', b',', b'inaudible',\n",
       "         b'for', b'scottish', b'business', b',', b'that', b's', b'not',\n",
       "         b'healthy', b'in', b'the', b'current', b'environment', b'and',\n",
       "         b'it', b's', b'certainly', b'not', b'going', b'to', b'contribute',\n",
       "         b'towards', b'growing', b'the', b'small', b'and', b'medium',\n",
       "         b'sized', b'businesses', b'in', b'particular', b'which', b'make',\n",
       "         b'up', b'98', b'%', b'of', b'all', b'our', b'business', b'here',\n",
       "         b'in', b'scot', b'text-end', b'padding-mark', b'padding-mark',\n",
       "         b'padding-mark', b'padding-mark', b'padding-mark']], dtype=object)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([sample_merge[0]['big5'] , sample_merge[0]['job'], sample_merge[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = tf.data.TextLineDataset([datapath+'vali.txt'])\n",
    "text_dataset = text_dataset.map(lambda x: tf.string_split([x], \" \").values)\n",
    "padding_info = [None]\n",
    "padding_value = tf.constant('padding-mark')\n",
    "dataset2=dataset2.padded_batch(4, padded_shapes = padding_info,padding_values=padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "    dataset = tf.data.TextLineDataset([datapath+'train.txt'])\n",
    "    dataset2 = dataset.map(lambda x: tf.string_split([x], \" \").values)\n",
    "    padding_info = [None]\n",
    "    padding_value = tf.constant('padding-mark')\n",
    "    dataset2=dataset2.padded_batch(4, padded_shapes = padding_info,padding_values=padding_value)\n",
    "    return dataset2.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=300\n",
    "def model_fn(features, mode):\n",
    "    feature_map = features\n",
    "    table = tf.contrib.lookup.index_table_from_file(vocabulary_file=PATH+'firstimpressionV2/tfrecord/text/word2ID.txt', num_oov_buckets=1)\n",
    "    ids = table.lookup(features)\n",
    "    \n",
    "    \n",
    "    embedding_matrix = tf.Variable(np.load(PATH+'firstimpressionV2/tfrecord/text/embedding_matrix.npy'),trainable=0)\n",
    "    padding_word = tf.Variable(tf.zeros([1,EMBEDDING_SIZE]),trainable=0)\n",
    "    UNK_vector = tf.Variable(np.load(PATH+'firstimpressionV2/tfrecord/text/UNK.npy'),trainable=0)\n",
    "    \n",
    "    embeddings = tf.concat(values=[embedding_matrix,padding_word,UNK_vector], axis=0)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, ids)\n",
    "    logits = tf.reduce_mean(input_tensor = tf.reduce_mean(input_tensor = embed,axis=-1),axis=-1)\n",
    "    h1 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "    loss = tf.losses.mean_squared_error(labels=logits, predictions=logits*h1)\n",
    "    predictions = {\n",
    "        'word':feature_map,\n",
    "        \"ids\": ids,\n",
    "        \"embed\":embed\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1)\n",
    "        train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'ok', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd953df7400>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "tester = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ok/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.16466e-06, step = 1\n",
      "INFO:tensorflow:global_step/sec: 275.946\n",
      "INFO:tensorflow:loss = 2.18662e-09, step = 101 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.949\n",
      "INFO:tensorflow:loss = 3.48568e-13, step = 201 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.651\n",
      "INFO:tensorflow:loss = 2.29785e-14, step = 301 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.084\n",
      "INFO:tensorflow:loss = 2.37005e-18, step = 401 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.017\n",
      "INFO:tensorflow:loss = 0.0, step = 501 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.058\n",
      "INFO:tensorflow:loss = 0.0, step = 601 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.545\n",
      "INFO:tensorflow:loss = 0.0, step = 701 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.358\n",
      "INFO:tensorflow:loss = 0.0, step = 801 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.795\n",
      "INFO:tensorflow:loss = 0.0, step = 901 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.173\n",
      "INFO:tensorflow:loss = 0.0, step = 1001 (0.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.978\n",
      "INFO:tensorflow:loss = 0.0, step = 1101 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.809\n",
      "INFO:tensorflow:loss = 0.0, step = 1201 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.532\n",
      "INFO:tensorflow:loss = 0.0, step = 1301 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 368.155\n",
      "INFO:tensorflow:loss = 0.0, step = 1401 (0.271 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1498 into ok/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fd953df79b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.train(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ok/model.ckpt-1498\n"
     ]
    }
   ],
   "source": [
    "OUT= list(tester.predict(input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = OUT[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed': array([[-0.0092, -0.0478, -0.038 , ...,  0.0041, -0.0209, -0.0178],\n",
       "        [-0.0069, -0.0343, -0.0589, ...,  0.2033,  0.0737, -0.0768],\n",
       "        [ 0.0156,  0.0752, -0.078 , ...,  0.0882, -0.0882, -0.0096],\n",
       "        ..., \n",
       "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]], dtype=float32),\n",
       " 'ids': array([    7,    15,    18,     9,   102,   145,  1412,   183,    10,\n",
       "            0,     2,    49,     3,    94,     1,    56,   317,    56,\n",
       "           10,   343,     0,    10,   251,    15,     6,   588,    13,\n",
       "         1258,    10,     2,    99,   294,    18,  1412,   183,    10,\n",
       "            0,    28,    10,    12,    36,     2,   153,    58,     1,\n",
       "            8, 13826, 13826, 13826, 13826, 13826, 13826, 13826, 13826, 13826]),\n",
       " 'word': array([b'text-start', b'...', b'is', b'a', b'little', b'bit', b'higher',\n",
       "        b'than', b'that', b',', b'i', b'want', b'to', b'say', b'.', b'as',\n",
       "        b'far', b'as', b'that', b'goes', b',', b'that', b'number', b'...',\n",
       "        b'the', b'amount', b'of', b'calories', b'that', b'i', b'actually',\n",
       "        b'eat', b'is', b'higher', b'than', b'that', b',', b'but', b'that',\n",
       "        b's', b'because', b'i', b'work', b'out', b'.', b'text-end',\n",
       "        b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "        b'padding-mark', b'padding-mark', b'padding-mark', b'padding-mark',\n",
       "        b'padding-mark'], dtype=object)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['...']-a0['embed'][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
